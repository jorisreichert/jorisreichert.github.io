<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>RU/BigData-Blog6</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Joris Reichert</a></h1>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="index.html">Home</a></li>
								<li><a href="generic.html">Generic</a></li>
								<li><a href="elements.html">Elements</a></li>
								<li><a href="#">Log In</a></li>
								<li><a href="#">Sign Up</a></li>
							</ul>
							<a href="#" class="close">Close</a>
						</div>
					</nav>

				<!-- Wrapper -->
					<section id="wrapper">
						<header>
							<div class="inner">
								<h2>Final Project</h2>
								<p>Blog #6 for the Big Data course.</p>
							</div>
						</header>

						<!-- Content -->
							<div class="wrapper">
								<div class="inner">
									<h2 class="major">Introduction
									</h2>
									<p>
									Welcome to my sixth and final blogpost in the Big Data series. This blog is not like any of the others: for this final assignment I am to do a project to get hands-on experience in running and debugging jobs and working on large amounts of unstructured data. Now that may not sound very different from my previous blogposts, but the nuance lies in the word <i>large</i>. Large no longer means <i>the entire works of Shakespeare</i>, but now it refers to <b>the entire Commoncrawl webcrawl</b>!
                  The project also differs from the way of work so far, as I am to
                  <ol>
                    <li>Run Spark code standalone, outside the notebook interface;</li>
                    <li>Scale up our workload in multiple steps, to tackle the issues that I encounter one by one.</li>
                  </ol>
                  Luckily, we're still provided with instructions (in three parts) to help us get started on the project.
                  </p>
                  <p>The code produced can be found on <a href="https://github.com/rubigdata/cc-2021-jorisreichert">this github page</a>, the assignments I followed can be found under <b>P: Final project</b>, parts I through III: <a href="https://rubigdata.github.io/course/index.html">here</a>.</p>

                  <h2 class="major">Instructions part 1: WARC files in Spark</h2>
                  <h3>Preliminaries</h3>
                  <p>
                  Sadly, it is time to retire our old and trusty docker image that we have been using in blogs 3 through 5. We're provided with something new, obtained and started by running:<pre><code>sudo docker pull rubigdata/course:project
sudo docker create --name cc -it -p 8080:8080 -p 9001:9001 -p 4040:4040 rubigdata/course:project
sudo docker start cc</code></pre>
                  Recall, on my system docker always requires sudo. Depending on your configuration you might not. Anyway, just like with our previous image we are now provdided with Zeppelin accessible from <code>http://localhost:9001/</code>.
                  </p>
                  <h3>Managing WARC files</h3>
                  <p>
                  To analyse how we can obtain data from WARC files we will look at the following piece of example code:</p>
                  <pre><code>// What are the text content-type records that were recorded in the crawl?
val wh = warcs.
        map{ wr => wr._2.getRecord() }.
        filter{ _.isHttp() }.
        map{ wr => (wr.getHeader().getUrl(),wr.getHttpHeaders().get("Content-Type")) }.
        filter{ 
            case(k,v) => v match { 
                case null => false
                case _ => v.startsWith("text") }
        }
wh.take(20).foreach{ println }</code></pre>
                  <p>
                  The first mapper is required to obtain the records of all warcs. Those records can still include records that contain images. We don't like images as they are harder to analyse, so we can use the filter function to check if the record is a HTTP (text) record.
                  </p><p>
                  The second mapper creates a tuple. The left half of the tuple opens the <b>WARC header</b> and obtains the URL from it. The right half of the header opens the <b>HTTP header</b> to obtain the content type from it.
                  </p><p>
                  The second filter filters out any tuples we created of which the content type (right side) does not start with the text "text". It seems like the only content-types are <code>text/html; charset=utf-8</code>, <code>text/css; charset=utf-8</code> and <code>null</code>. The last of those is filtered out, presumably those are HTTP requests as their content type is <i>null</i> and leaves us with only the HTTP responses. 
                  </p>
                  <h3>Parsing HTML</h3>
                  <p>
                  Once we have the HTML body (obtained by mapping <code>.getHttpStringBody()</code> on our warc records) we can use <code>Jsoup</code> to parse the HTML. Note that using "clean Jsoup code" with <code>def</code> can give Spark problems, which is most easy to work around by "inlining" more of your processing. Usefull references are <a href="https://jsoup.org/">the Jsoup website</a> and <a href="https://www.lihaoyi.com/post/ScrapingWebsitesusingScalaandJsoup.html">this blogpost</a>, the latter of which has some examples of using Jsoup in plain scala.
                  </p>
                  <p>
                  Here is some example code which creates and prints an RDD with tuples of the <i>title</i> of a page and each link on that page (the <code>href</code> destination of each <code>a</code> element in the page).
                  <pre><code>import org.jsoup.Jsoup
import org.jsoup.nodes.{Document,Element}
import collection.JavaConverters._

val wb = warcs.map{ wr => wr._2.getRecord().getHttpStringBody()}.
               map{ wb => {
                        val d = Jsoup.parse(wb)
                        val t = d.title()
                        val links = d.select("a").asScala
                        links.map(l => (t,l.attr("href"))).toIterator
                    }
                }.
                flatMap(identity)

// Inspect data:
wb.take(50).foreach(println)</code></pre>
                  </p>
                  <p>
                  We could slightly alter the <code>links.map(...)</code> line to also add the anchor text to the tuple.
<pre><code>links.map(l => (t,l.attr("href"),l.text())).toIterator</code></pre>
                  Anchor texts can be usefull. If I recall correctly, Wikipedia uses anchor texts of links that refer to their pages as a way of "determining/describing what the page is about". For example, in our data fragment two of our results are:
                  <pre>(Course Information,assignments/A1-preliminaries.html,A1)
(Course Information,assignments/A1-preliminaries.html,Blogging assignment)</pre>
                  Both "A1" and "Blogging assignment" are used to describe the hyperlink. As we know, the page is indeed a blogging assignment and we also have referred to it as "A1" (short for "Assignment 1").
                  </p>

                  <h2 class="major">Instructions part 2: Standalone Program</h2>
                  <p>
                  So far we have used Spark inside Zeppelin Notebooks, in a very user-friendly and interactive setting. As the concept of interactive notebooks does not work well with the concept of a cluster shared by many people, we want to convert our Zeppelin Notebook to a standalone application that we can run on the cluster. 
                  </p>

                  <p>
                  While our container (<code>cc</code>) is still running, we can use <code>sudo docker exec -it cc /bin/bash</code> to obtain a shell inside our docker container. In this container we navigate to a folder called <code>rubigdata</code> that I've also added to <a href="https://github.com/rubigdata/cc-2021-jorisreichert"> my github</a>. Inside, we find the two inputs required to build a trivial sample app. The scala code is found in <code>RUBigDataApp.scala</code> and the information on how it can be built into a self-contained app can be found in <code>build.sbt</code>. The <code>target</code> subfolder contains the standalone app that we created, which can be executed by running: <pre><code>spark-submit target/scala-2.12/rubigdataapp_2.12-1.0.jar</code></pre>
                  </p>

                  <p>
                  We are provided with some commands to make a <i>fat jar</i> (a java archive that includes all its dependencies) of the sample app. How this all works when you have a cluster to work with will be discussed in Instructions part 3. After that, we will write our own small app, build it, deploy it to the cluster and reason about the results.

                  <h2 class="major">Instructions part 3: Cluster REDBAD</h2>
                  <p>
                  On to bigger and better things: the educational REDBAD cluster. We have to establish a vpn connection to the university to use this cluster. Configuring it wasn't too hard. For personal future reference: we can start it from our user directory using <code>sudo openvpn openvpn-ca-science.ovpn</code>.
                  </p>
                  <p>
                  The cc docker container was short lived as we again have to use a new container for working with the cluster. We will keep it up for now though, as viewing through and working in the notebook might prove usefull along the line. First, we set up our github username in an environment variable and then we create and start the docker container.
                  <pre><code>export GITHUB_USERNAME="jorisreichert"
sudo docker create --name redbad -e HADOOP_USER_NAME=${GITHUB_USERNAME} -it rubigdata/redbad
sudo docker start redbad
                  </code></pre>
                  Now we can attach to the cluster and obtain shell access using <code>docker attach redbad</code>.
                  </p>
                  <p>
                  As with many parts of this course, we are again provided with a beautifully well-prepared docker environment. I suppose it makes sense as I imagine there will only be need for a few people setting up clusters while we are being taught how to use those environments. Anyway, let's get started with running our example program on the cluster.
                  </p>
                  <p>
                  <pre><code>hdfs dfs -put rubigdata-test.txt /user/${HADOOP_USER_NAME}
hdfs dfs -ls /user/${HADOOP_USER_NAME}</code></pre>
                  This puts our testfile in hdfs and lets us check if it's really there. Next we edit the example program to use the textfile we just put in hdfs.
                  <pre><code>nano src/main/scala/org/rubigdata/RUBigDataApp.scala</code></pre>
                  It's hidden in a few folders, but we found it. This scala program reads its data from a filename stored in variable fnm, which we have adapted to:
                  <pre><code> val fnm = s"hdfs:///user/jorisreichert/rubigdata-test.txt"</code></pre>
                  For our real app we should of course use the data found at <code>hdfs:///single-warc-segment</code>.
                  After our adaptation we can build and submit our program very similarly to what we've done in Part 2. 
                  <pre><code>sbt assembly
spark-submit --deploy-mode cluster --queue default target/scala-2.12/RUBigDataApp-assembly-1.0.jar</code></pre>
                  The output can be found <a href="http://rbdata01.cs.ru.nl:19888/jobhistory/logs/rbdata03:38547/container_e02_1623272363921_1667_01_000001/container_e02_1623272363921_1667_01_000001/jorisreichert/stdout?start=-4096">here</a>, where we got by navigating to <a href="http://rbdata01.cs.ru.nl:18080/">http://rbdata01.cs.ru.nl:18080/</a>, clicking our app ID, going to the 'Executors' tab, and click on the 'stdout' log of our driver (the Executor with Executor ID 'driver').
                  </p>
                  <h2 class="major">Part 4: scraping some mailaddresses</h2>
                  <p>
                  Now, after having learned how we can deploy a basic scala program to the cluster, it's time for some real work. Now, what kind of program would be useful?
                  </p><p>
                  Some time ago I was part of a workgroup that attempted to improve the mental wellbeing of students by searching some students that were willing to aid and meet up with any students that had difficulties in life without really knowing who to talk to. For this project we set up a webpage so people could get in touch with the 'aiding students'. After some time however, all the 'aiding students' got a huge influx in spam email. We suspected their mailaddresses had been scraped from the website, which got me interested in trying this on the webcrawl. If all goes well we might be able to expand in any of the following ways:
                  <ul>
                  <li>Rank occurring email addresses by domain name</li>
                  <li>Compare usage of 'own-domain' email addresses within a domain to more generic email providers (e.g. compare the occurrence of @ru.nl and @gmail.com within the webpages of the ru domain)</li>
                  <li>calculate the maximum and average amount of email addresses on webpages</li>
                  </ul>
                  </p>
                  <p>
                  So, we've wrote some scala code. We put it in the same folder as our <code>RUBigDataApp.scala</code> our hdfs by using / pasting it in nano. In order to build it, we've changed the first line of  <code>build.sbt</code> from <code>name            := "RUBigDataApp"</code> to <code>name            := "GimmeDemMails"</code> and hope this works.
                  </p>
                  <h3>Version 1: Opening a WARC file on the cluster and counting the warc records.</h3>
                  <p>
                  In <code>GimmeDemMails.scala</code> we wrote the following base program that we will improve on iteratively. It's in image format partly to disencourage plagiarism and partly because of readability provided by nano's colouring.
                  <a href="#" class="image"><img src="images/bigdata-project-01-sanity-check.png" style="max-width:100%" alt="" /></a>
                  </p>
                  <p>
                  The program was compiled and submitted to the cluster. The output can be found <a href="http://rbdata01.cs.ru.nl:19888/jobhistory/logs/rbdata02:38689/container_e02_1623272363921_1698_01_000001/container_e02_1623272363921_1698_01_000001/jorisreichert/stdout?start=-4096">here</a>.
                  </p>
                  <h3>Version 2: Taking some a href targets with context</h3>
                  <p>Next we cleverly combine the code we used in part one to make a triple of (website the link occurred on, the target of the href, the text used to describe the link). Of course we commented out the sanity check.
                  <a href="#" class="image"><img src="images/bigdata-project-02-triple.png" style="max-width:100%" alt="" /></a>
                  <i>Sidenote on the workflow explaining the change of editor: As working with nano became tedious I used gedit from here onwards and just pasted all the code in the nano editor. Small compilation errors were then fixed in nano.</i>
                  </p>
                  <p>
                  This program was also compiled and submitted to the cluster. The output can be found <a href="http://rbdata01.cs.ru.nl:19888/jobhistory/logs/rbdata09:35353/container_e02_1623272363921_1714_01_000001/container_e02_1623272363921_1714_01_000001/jorisreichert/stdout/?start=0&start.time=0&end.time=9223372036854775807">here</a>.
                  </p>
                  <h3>Version 3: Filtering for email addresses</h3>
                  <p>
                  Directly following on <code>flatMap(identity)</code> I added:
                  <pre><code>.
        filter{case (u,a,t) => a matches "$@%.$"}</code></pre>
                  </p><p>
                  This filter with a regex should filter out all triples of which the link does not point to a mail address. This program was also compiled and submitted to the cluster. <a href="http://rbdata01.cs.ru.nl:18080/history/application_1623272363921_1787/1/jobs/">This job</a> however gave some more 'interesting' results. <a href="http://rbdata01.cs.ru.nl:19888/jobhistory/logs/rbdata03:38547/container_e02_1623272363921_1787_01_000001/container_e02_1623272363921_1787_01_000001/jorisreichert/stdout?start=-4096">The output</a> is empty...
                  </p><p>
                  Our regex seems to be fine when tested against a few examples however, so this would lead us to assume this WARC file did not include any pages that had a clickable mail link like <code>a href = "mailto:abc@example.com"</code>. The next step would be to either check the entire body of webpages or to check more than one warc file at a time.





                  <h4><i>For now though, I think this concludes what is expected of us for this practical assignment.</i></h4>

  									<!-- <a href="#" class="image"><img src="images/Spark-SQL-stream-active-structured.png"/></a> -->
  										<!-- <a href="#" class="image"><img src="images/Spark-SQL-mace-vs-scimmy.png" style="max-width:100%"></a> -->
  									
      						<i>If I misunderstood this I'll happily continue working on this for the resit. If not I might continue working on this anyway (after the deadline, as long as the cluster remains available) because I found it pretty interesting and enjoyable to do something with the knowledge I gained in FP.
      						<!-- <h4>To the peers giving feedback: I really appreciate extensive/in-depth feedback!! :D</h4> -->






									<!-- Referring to other articles -->
									<!-- <section class="features">
										<article>
											<a href="#" class="image"><img src="images/pic04.jpg" alt="" /></a>
											<h3 class="major">Sed feugiat lorem</h3>
											<p>Lorem ipsum dolor sit amet, consectetur adipiscing vehicula id nulla dignissim dapibus ultrices.</p>
											<a href="#" class="special">Learn more</a>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic05.jpg" alt="" /></a>
											<h3 class="major">Nisl placerat</h3>
											<p>Lorem ipsum dolor sit amet, consectetur adipiscing vehicula id nulla dignissim dapibus ultrices.</p>
											<a href="#" class="special">Learn more</a>
										</article>
									</section> -->

								</div>
							</div>

					</section>

				<!-- Footer -->
					<section id="footer">
						<div class="inner">
							<!-- <h2 class="major">Get in touch</h2>
							<p>Cras mattis ante fermentum, malesuada neque vitae, eleifend erat. Phasellus non pulvinar erat. Fusce tincidunt, nisl eget mattis egestas, purus ipsum consequat orci, sit amet lobortis lorem lacus in tellus. Sed ac elementum arcu. Quisque placerat auctor laoreet.</p>
							<form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="email" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="4"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form> -->
							<ul class="contact">
								<!-- <li class="icon solid fa-home">
									Untitled Inc<br />
									1234 Somewhere Road Suite #2894<br />
									Nashville, TN 00000-0000
								</li> -->
								<!-- <li class="icon solid fa-envelope"><a href="#">information@untitled.tld</a></li> -->
								<li class="icon brands fa-linkedin"><a href="#">linkedin.com/untitled-tld</a></li>
								<li class="icon brands fa-github"><a href="http://github.com/jorisreichert">github.com/jorisreichert</a></li>
							</ul>
							<ul class="copyright">
								<li>&copy; Joris Reichert. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>